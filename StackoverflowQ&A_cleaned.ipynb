{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569,
     "referenced_widgets": [
      "c8a90a7b81e74334a1f43d144f9d4b6c",
      "6b65f0a72c0e41d58f018ac287d0ecf9",
      "524757a2fee94114af729db4da413b3e",
      "37ecdad20dff42b19123990268ae83fe",
      "88d5ac1745504315b3f2049b9197a0ee",
      "a59ef7c6cf8d4d178c62b0f41860d52b",
      "b35568450d25472bb1e72f4078e6493c",
      "8082ef13468d4e1c9a59059b99d0642b",
      "2f4edc9fb2a24619adde7cb3f612f72d",
      "8630e53ee92547e49e6b1e88143f8bc1",
      "1ce580a386914c04b2012b7c1b246258",
      "24a918773a204917baaa2b831f28057a",
      "0997efa1121046159bbdb9d704924069",
      "25762927128d4bb094c094e219fe1e92",
      "d82d86ef544c4834989fb927f8ef96af",
      "a2a5c0f13c3d4205b44d804fdec47793",
      "70aeea7ef05343158783eb6e93954fd8",
      "37635f52678543848d7ab61d3d681c3a",
      "eb78b448afc24c75933ac972ef1e2b07",
      "1917230d5e224b24a6c45b79fa8b18e8",
      "78184c461f47447f9a9f1a3c407a2e3c",
      "50cc5e73b6cb4c8990b2c9c52760fd4b"
     ]
    },
    "id": "QG3fbQ0p0lQi",
    "outputId": "d8ceb6f0-e9c2-4ff1-9466-3ead679d3271"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-a532739c-11df-4e53-9966-955b1869f961\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-a532739c-11df-4e53-9966-955b1869f961\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving commandline_qa.json to commandline_qa (6).json\n",
      "Loaded 180 Q&A pairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a90a7b81e74334a1f43d144f9d4b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a918773a204917baaa2b831f28057a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 125,788,416 || trainable%: 0.4689\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 19:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.722800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.263200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.974600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.702200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.391000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LoRA adapter saved to ./lora-gptneo\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets peft accelerate\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "import json\n",
    "with open(\"commandline_qa.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts = [f\"Q: {qa['question']}\\nA: {qa['answer']}\" for qa in data]\n",
    "print(f\"Loaded {len(texts)} Q&A pairs\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-gptneo\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=3e-4,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./lora-gptneo\")\n",
    "print(\"âœ… LoRA adapter saved to ./lora-gptneo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CrsgyplBoWR",
    "outputId": "900f35c4-09eb-4079-f52f-89e6dfe69abb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2336\n",
      "-rw-r--r-- 1 root root     779 Jun 17 12:58 adapter_config.json\n",
      "-rw-r--r-- 1 root root 2365872 Jun 17 12:58 adapter_model.safetensors\n",
      "drwxr-xr-x 2 root root    4096 Jun 17 12:58 checkpoint-90\n",
      "-rw-r--r-- 1 root root    5097 Jun 17 12:58 README.md\n",
      "-rw-r--r-- 1 root root    5240 Jun 17 12:58 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./lora-gptneo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dbf03qssBtmZ",
    "outputId": "662c3ab8-f937-449a-ab1c-9a7046b0f16b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoForCausalLM(\n",
       "      (transformer): GPTNeoModel(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(2048, 768)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPTNeoBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTNeoAttention(\n",
       "              (attention): GPTNeoSelfAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPTNeoMLP(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "base_model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model = PeftModel.from_pretrained(base_model, \"./lora-gptneo\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qs9b3Vu_CJoJ",
    "outputId": "b875393a-1fc5-46db-8bcb-0987639f02a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model 'EleutherAI/gpt-neo-125M' and tokenizer...\n",
      "Loading LoRA adapter...\n",
      "\n",
      "Generated steps:\n",
      "Step 1: 1. Create a new Git branch\n",
      "Step 2: 2. Select the Git branch you want to create\n",
      "Step 3: 3. Select the Git branch you want to create\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "def is_shell_command(line):\n",
    "    shell_cmd_starts = (\n",
    "        \"cd \", \"ls\", \"git \", \"mkdir\", \"rm\", \"echo\", \"touch\",\n",
    "        \"python\", \"./\", \"sudo\", \"cat \", \"cp \", \"mv \", \"pwd\"\n",
    "    )\n",
    "    return line.strip().startswith(shell_cmd_starts)\n",
    "\n",
    "def main():\n",
    "    instruction = \"Create a new Git branch and switch to it\"\n",
    "    base_model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "    print(f\"Loading base model '{base_model_name}' and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    print(\"Loading LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(base_model, \"./lora-gptneo\")\n",
    "    model.eval()\n",
    "    prompt = f\"Instruction: {instruction}\\nSteps:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    steps_text = generated_text.split(\"Steps:\")[-1].strip()\n",
    "    steps = [line.strip() for line in steps_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    log_path = \"logs/trace.jsonl\"\n",
    "\n",
    "    log_entries = []\n",
    "\n",
    "    print(\"\\nGenerated steps:\")\n",
    "    for idx, step in enumerate(steps, start=1):\n",
    "        dry_run = False\n",
    "        if idx == 1 and is_shell_command(step):\n",
    "            dry_run = True\n",
    "            print(f\"(Dry-run) Shell command: {step}\")\n",
    "        else:\n",
    "            print(f\"Step {idx}: {step}\")\n",
    "\n",
    "        log_entries.append({\"step\": idx, \"text\": step, \"dry_run\": dry_run})\n",
    "    with open(log_path, \"a\") as f:\n",
    "        for entry in log_entries:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvIlT2QID3Zk",
    "outputId": "6d8fe7ca-e158-4234-e950-d1cf07618c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter folder './lora-gptneo' found!\n",
      "Files: ['README.md', 'checkpoint-90', 'adapter_model.safetensors', 'training_args.bin', 'adapter_config.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"./lora-gptneo\"):\n",
    "    print(\"LoRA adapter folder './lora-gptneo' found!\")\n",
    "    print(\"Files:\", os.listdir(\"./lora-gptneo\"))\n",
    "else:\n",
    "    print(\"LoRA adapter folder './lora-gptneo' NOT found. Please train and save the adapter first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfHJfGo_D_Tu",
    "outputId": "95cc5655-0a59-48e9-9e56-a49e0090dbac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output:\n",
      "\n",
      "Instruction: Create a new Git branch and switch to it\n",
      "Steps:\n",
      "\n",
      "1. Create a new Git branch\n",
      "2. Select the Git branch you want to create\n",
      "3. Select the Git branch you want to create\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "base_model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model = PeftModel.from_pretrained(base_model, \"./lora-gptneo\")\n",
    "model.eval()\n",
    "\n",
    "instruction = \"Create a new Git branch and switch to it\"\n",
    "prompt = f\"Instruction: {instruction}\\nSteps:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated output:\\n\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jw13XH42EaEs",
    "outputId": "59a791a3-d756-4ab5-b74a-902ecd68f1c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 13:26:40.435169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750166800.878780   17062 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750166801.040408   17062 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "âœ… Loading fine-tuned LoRA adapter...\n",
      "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "\n",
      "ðŸ“‹ Generated Steps:\n",
      "Step 1: 1. Create a new Git branch\n",
      "Step 2: 2. Select the Git branch you want to create\n",
      "Step 3: 3. Select the Git branch you want to create\n"
     ]
    }
   ],
   "source": [
    "!python agent.py \"Create a new Git branch and switch to it\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miyXimcjFC10"
   },
   "outputs": [],
   "source": [
    "eval_prompts = [\n",
    "    \"Create a new Git branch and switch to it.\",\n",
    "    \"Compress the folder reports into reports.tar.gz.\",\n",
    "    \"List all Python files in the current directory recursively.\",\n",
    "    \"Set up a virtual environment and install requests.\",\n",
    "    \"Fetch only the first ten lines of a file named output.log.\",\n",
    "\n",
    "    \"Delete all .log files older than 7 days in /var/log.\",\n",
    "    \"Rename all .jpeg files in the current directory to .jpg.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5Hh_LIwFUEM",
    "outputId": "38889a97-84d1-48fa-f507-b56a69b2e151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f0c692c4d68db249344ec8a89cb93b0555053f78ca1d50a8aed06c26c1349834\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8skmvHtkFO7-",
    "outputId": "caacde95-5f79-431a-dd72-843eb0e674f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eval_static.md saved.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from rouge_score import rouge_scorer\n",
    "import os\n",
    "\n",
    "base_model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "\n",
    "fine_tuned = PeftModel.from_pretrained(base_model, \"./lora-gptneo\")\n",
    "fine_tuned.eval()\n",
    "\n",
    "def format_prompt(instr):\n",
    "    return f\"Instruction: {instr}\\nSteps:\"\n",
    "\n",
    "# Generate output\n",
    "def generate(model, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"Steps:\")[-1].strip()\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "results = []\n",
    "\n",
    "for i, prompt in enumerate(eval_prompts, 1):\n",
    "    formatted = format_prompt(prompt)\n",
    "    base_output = generate(base_model, formatted)\n",
    "    tuned_output = generate(fine_tuned, formatted)\n",
    "    score = scorer.score(base_output, tuned_output)['rougeL'].fmeasure\n",
    "    results.append((prompt, base_output, tuned_output, score))\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "with open(\"eval_static.md\", \"w\") as f:\n",
    "    f.write(\"# ðŸ“Š Static Evaluation: Base vs Fine-Tuned Outputs\\n\\n\")\n",
    "    for idx, (prompt, base_out, tuned_out, rougeL) in enumerate(results, 1):\n",
    "        f.write(f\"## Prompt {idx}: {prompt}\\n\\n\")\n",
    "        f.write(f\"**Base Model Output:**\\n```\\n{base_out}\\n```\\n\")\n",
    "        f.write(f\"**Fine-Tuned Output:**\\n```\\n{tuned_out}\\n```\\n\")\n",
    "        f.write(f\"**ROUGE-L Score:** `{rougeL:.4f}`\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "print(\"âœ… eval_static.md saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "6f4efec2032242e58f67d2f103c75098",
      "1594a7acf9db4c1b84dee8042d6500e6",
      "d5bc51eab48944ad881bc3dcf65f71a5",
      "9e56716cbe924e5899e40506fa7d1d4d",
      "054ef1af6fe846d0bff786b042d56ba5",
      "376946a6e9cf435d8419cdedbf064606",
      "5f95c1911a6a4def83698e50853e641a",
      "a7836f87407c4690b4fe02063f2d6f72",
      "a6a83dd51bcf4400b034e98f0140a0cd",
      "986a5e9f3df7465f9e5d62050cb5c965",
      "f10d8d83d0b84b9d80798f02869ef7fc",
      "fcb5d89103874803999b22faacc97e9a",
      "ed71f318d6e043ec8da7bd912355782c",
      "f77dc3fed7bc4b8c805702734d7c7713",
      "5fc32aa2cb8546cba8486a51101ac72c",
      "ad481c3792604b9ab37ecdaafcc594c3",
      "3f52c49ef0d64859b0913bd8f93620fd",
      "626c7dd11c3449d199cbd65fbf7a8e52",
      "e785578d7c3f4e4abc26911032b760d3",
      "d40e41368ba44f03912e4e21cf50d0c8",
      "377aa30c41ff43359efc4705c805b44e",
      "6d65907cabed44c8b7f90f3b0049435c"
     ]
    },
    "id": "bqU2baykMfcF",
    "outputId": "32571baa-fa9a-4904-eb1e-22d6046b91b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 180 Q&A pairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4efec2032242e58f67d2f103c75098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb5d89103874803999b22faacc97e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "import json\n",
    "with open(\"commandline_qa.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts = [f\"Q: {qa['question']}\\nA: {qa['answer']}\" for qa in data]\n",
    "print(f\"Loaded {len(texts)} Q&A pairs\")\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "uJMaxnTZOWGn",
    "outputId": "97b9f9d2-17b3-47e9-8cf8-217d1a287e4f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 29:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.430400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.277200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LoRA adapter saved to ./lora-gptneo\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-gptneo\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=3e-4,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./lora-gptneo\")\n",
    "print(\"âœ… LoRA adapter saved to ./lora-gptneo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af-i_k42VZpm",
    "outputId": "e0d91589-a658-4eac-cd55-92c7cabe82bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model outputs:\n",
      "\n",
      "Prompt: Create a new Git branch and switch to it.\n",
      "Instruction: Create a new Git branch and switch to it.\n",
      "Steps:\n",
      "1. Create a new Git branch.\n",
      "2. Select the Git branch you want to create.\n",
      "3. Select the Git branch you want to switch to.\n",
      "\n",
      "Prompt: Compress the folder reports into reports.tar.gz.\n",
      "Instruction: Compress the folder reports into reports.tar.gz.\n",
      "Steps:\n",
      "\n",
      "\n",
      "Prompt: List all Python files in the current directory recursively.\n",
      "Instruction: List all Python files in the current directory recursively.\n",
      "Steps:\n",
      "\n",
      "\n",
      "Prompt: Set up a virtual environment and install requests.\n",
      "Instruction: Set up a virtual environment and install requests.\n",
      "Steps:\n",
      "\n",
      "1. Install the virtual environment\n",
      "2. Install the virtual environment\n",
      "3. Install the virtual environment\n",
      "\n",
      "\n",
      "Prompt: Fetch only the first ten lines of a file named output.log.\n",
      "Instruction: Fetch only the first ten lines of a file named output.log.\n",
      "Steps:\n",
      "\n",
      "\n",
      "Prompt: Your edge case 1\n",
      "Instruction: Your edge case 1\n",
      "Steps:\n",
      "1. Create a new edge case 1\n",
      "2. Create a new edge case 2\n",
      "3. Create a new edge case 3\n",
      "4. Create a new edge case 4\n",
      "5. Create a new edge case 5\n",
      "6. Create a new edge case 6\n",
      "7. Create a new edge case 7\n",
      "8. Create a new edge case 8\n",
      "9. Create a new edge case 9\n",
      "10. Create a new edge case 10\n",
      "11. Create a new edge case 11\n",
      "12. Create a new edge case 12\n",
      "13. Create a new edge case 13\n",
      "14. Create a new edge case 14\n",
      "15. Create a new edge case 15\n",
      "16. Create a new edge case 16\n",
      "17. Create a new edge\n",
      "\n",
      "Prompt: Your edge case 2\n",
      "Instruction: Your edge case 2\n",
      "Steps:\n",
      "1. Create a new edge case 2\n",
      "2. Add a new edge case 2\n",
      "3. Add a new edge case 2\n",
      "4. Add a new edge case 2\n",
      "5. Add a new edge case 2\n",
      "6. Add a new edge case 2\n",
      "7. Add a new edge case 2\n",
      "8. Add a new edge case 2\n",
      "9. Add a new edge case 2\n",
      "10. Add a new edge case 2\n",
      "\n",
      "\n",
      "Fine-tuned Model outputs:\n",
      "\n",
      "Prompt: Create a new Git branch and switch to it.\n",
      "Instruction: Create a new Git branch and switch to it.\n",
      "Steps:\n",
      "\n",
      "Step 1:\n",
      "\n",
      "Step 2:\n",
      "\n",
      "Step 3:\n",
      "\n",
      "Step 4:\n",
      "\n",
      "Step 5:\n",
      "\n",
      "Step 6:\n",
      "\n",
      "Step 7:\n",
      "\n",
      "Step 8:\n",
      "\n",
      "Step 9:\n",
      "\n",
      "Step 10:\n",
      "\n",
      "Step 11:\n",
      "\n",
      "Step 12:\n",
      "\n",
      "Step 13:\n",
      "\n",
      "Step 14:\n",
      "\n",
      "Step 15:\n",
      "\n",
      "Step 16:\n",
      "\n",
      "Step 17:\n",
      "\n",
      "Step 18:\n",
      "\n",
      "Step 19:\n",
      "\n",
      "Step 20:\n",
      "\n",
      "Step 21:\n",
      "\n",
      "Step 22:\n",
      "\n",
      "Step 23:\n",
      "\n",
      "Step 24:\n",
      "\n",
      "Step 25:\n",
      "\n",
      "Step 26:\n",
      "\n",
      "Step 27:\n",
      "\n",
      "Step 28:\n",
      "\n",
      "Step 29:\n",
      "\n",
      "Step 30:\n",
      "\n",
      "\n",
      "Prompt: Compress the folder reports into reports.tar.gz.\n",
      "Instruction: Compress the folder reports into reports.tar.gz.\n",
      "Steps:\n",
      "\n",
      "Step 1: Extract the directory from the output of the command\n",
      "Step 2: Extract the directory from the output of the command\n",
      "Step 3: Extract the directory from the output of the command\n",
      "Step 4: Extract the directory from the output of the command\n",
      "Step 5: Extract the directory from the output of the command\n",
      "Step 6: Extract the directory from the output of the command\n",
      "Step 7: Extract the directory from the output of the command\n",
      "Step 8: Extract the directory from the output of the command\n",
      "Step 9: Extract the directory from the output of the command\n",
      "Step 10: Extract the directory from the output of the command\n",
      "Step 11: Extract the directory from the output of the command\n",
      "Step 12: Extract the directory\n",
      "\n",
      "Prompt: List all Python files in the current directory recursively.\n",
      "Instruction: List all Python files in the current directory recursively.\n",
      "Steps:\n",
      "\n",
      "1. Recursively list all Python files in the current directory recursively.\n",
      "2. Recursively list all Python files in the current directory recursively.\n",
      "3. Recursively list all Python files in the current directory recursively.\n",
      "4. Recursively list all Python files in the current directory recursively.\n",
      "5. Recursively list all Python files in the current directory recursively.\n",
      "6. Recursively list all Python files in the current directory recursively.\n",
      "7. Recursively list all Python files in the current directory recursively.\n",
      "8. Recursively list all Python files in the current directory recursively.\n",
      "9. Recursively\n",
      "\n",
      "Prompt: Set up a virtual environment and install requests.\n",
      "Instruction: Set up a virtual environment and install requests.\n",
      "Steps:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install:\n",
      "\n",
      "Install\n",
      "\n",
      "Prompt: Fetch only the first ten lines of a file named output.log.\n",
      "Instruction: Fetch only the first ten lines of a file named output.log.\n",
      "Steps:\n",
      "\n",
      "Step 1: Extract the first line of the file named output.log\n",
      "Step 2: Extract the first line of the file named output.log\n",
      "Step 3: Extract the first line of the file named output.log\n",
      "Step 4: Extract the first line of the file named output.log\n",
      "Step 5: Extract the first line of the file named output.log\n",
      "Step 6: Extract the first line of the file named output.log\n",
      "Step 7: Extract the first line of the file named output.log\n",
      "Step 8: Extract the first line of the file named output.log\n",
      "Step 9: Extract the first line of the file named output.log\n",
      "Step 10: Extract the first line of the file named output.log\n",
      "\n",
      "Prompt: Your edge case 1\n",
      "Instruction: Your edge case 1\n",
      "Steps:\n",
      "\n",
      "Step 1:\n",
      "\n",
      "Step 2:\n",
      "\n",
      "Step 3:\n",
      "\n",
      "Step 4:\n",
      "\n",
      "Step 5:\n",
      "\n",
      "Step 6:\n",
      "\n",
      "Step 7:\n",
      "\n",
      "Step 8:\n",
      "\n",
      "Step 9:\n",
      "\n",
      "Step 10:\n",
      "\n",
      "Step 11:\n",
      "\n",
      "Step 12:\n",
      "\n",
      "Step 13:\n",
      "\n",
      "Step 14:\n",
      "\n",
      "Step 15:\n",
      "\n",
      "Step 16:\n",
      "\n",
      "Step 17:\n",
      "\n",
      "Step 18:\n",
      "\n",
      "Step 19:\n",
      "\n",
      "Step 20:\n",
      "\n",
      "Step 21:\n",
      "\n",
      "Step 22:\n",
      "\n",
      "Step 23:\n",
      "\n",
      "Step 24:\n",
      "\n",
      "Step 25:\n",
      "\n",
      "Step 26:\n",
      "\n",
      "Step 27:\n",
      "\n",
      "Step 28:\n",
      "\n",
      "Step 29:\n",
      "\n",
      "Step 30:\n",
      "\n",
      "\n",
      "Prompt: Your edge case 2\n",
      "Instruction: Your edge case 2\n",
      "Steps:\n",
      "\n",
      "Step 1:\n",
      "\n",
      "Step 2:\n",
      "\n",
      "Step 3:\n",
      "\n",
      "Step 4:\n",
      "\n",
      "Step 5:\n",
      "\n",
      "Step 6:\n",
      "\n",
      "Step 7:\n",
      "\n",
      "Step 8:\n",
      "\n",
      "Step 9:\n",
      "\n",
      "Step 10:\n",
      "\n",
      "Step 11:\n",
      "\n",
      "Step 12:\n",
      "\n",
      "Step 13:\n",
      "\n",
      "Step 14:\n",
      "\n",
      "Step 15:\n",
      "\n",
      "Step 16:\n",
      "\n",
      "Step 17:\n",
      "\n",
      "Step 18:\n",
      "\n",
      "Step 19:\n",
      "\n",
      "Step 20:\n",
      "\n",
      "Step 21:\n",
      "\n",
      "Step 22:\n",
      "\n",
      "Step 23:\n",
      "\n",
      "Step 24:\n",
      "\n",
      "Step 25:\n",
      "\n",
      "Step 26:\n",
      "\n",
      "Step 27:\n",
      "\n",
      "Step 28:\n",
      "\n",
      "Step 29:\n",
      "\n",
      "Step 30:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Create a new Git branch and switch to it.\",\n",
    "    \"Compress the folder reports into reports.tar.gz.\",\n",
    "    \"List all Python files in the current directory recursively.\",\n",
    "    \"Set up a virtual environment and install requests.\",\n",
    "    \"Fetch only the first ten lines of a file named output.log.\",\n",
    "\n",
    "    \"Your edge case 1\",\n",
    "    \"Your edge case 2\",\n",
    "]\n",
    "\n",
    "def generate_steps(model, tokenizer, prompt):\n",
    "    input_text = f\"Instruction: {prompt}\\nSteps:\\n\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Base Model outputs:\")\n",
    "for p in prompts:\n",
    "    print(f\"\\nPrompt: {p}\")\n",
    "    print(generate_steps(base_model, tokenizer, p))\n",
    "\n",
    "print(\"\\nFine-tuned Model outputs:\")\n",
    "for p in prompts:\n",
    "    print(f\"\\nPrompt: {p}\")\n",
    "    print(generate_steps(model, tokenizer, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F00wP8qSYU2J",
    "outputId": "4998f304-ffe8-4976-8237-d49568e0a7ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent.py\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "def is_shell_command(line):\n",
    "    shell_cmd_starts = (\n",
    "        \"cd \", \"ls\", \"git \", \"mkdir\", \"rm\", \"echo\", \"touch\",\n",
    "        \"python\", \"./\", \"sudo\", \"cat \", \"cp \", \"mv \", \"pwd\"\n",
    "    )\n",
    "    return line.strip().startswith(shell_cmd_starts)\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"âŒ Usage: python agent.py \\\"<your instruction>\\\"\")\n",
    "        return\n",
    "\n",
    "    instruction = sys.argv[1]\n",
    "\n",
    "    base_model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "    print(\"â³ Loading tokenizer and base model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "\n",
    "    print(\"âœ… Loading fine-tuned LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(base_model, \"./lora-gptneo\")\n",
    "    model.eval()\n",
    "\n",
    "    # Few-shot style prompt with example to guide output format\n",
    "    prompt = f\"\"\"\n",
    "Instruction: Initialize a new Git repo and push code to GitHub\n",
    "Steps:\n",
    "1. git init\n",
    "2. git add .\n",
    "3. git commit -m \"Initial commit\"\n",
    "4. git branch -M main\n",
    "5. git remote add origin <repo_url>\n",
    "6. git push -u origin main\n",
    "\n",
    "Instruction: {instruction}\n",
    "Steps:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the steps after last \"Steps:\" occurrence\n",
    "    steps_text = generated_text.split(\"Steps:\")[-1].strip()\n",
    "    # Filter and clean lines that look like numbered steps\n",
    "    steps = []\n",
    "    for line in steps_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        # Accept lines that start with a number and dot like \"1. git init\"\n",
    "        if line and (line[0].isdigit() and line[1] == \".\"):\n",
    "            steps.append(line)\n",
    "\n",
    "    if not steps:\n",
    "        print(\"âš ï¸ No steps were generated.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    log_path = \"logs/trace.jsonl\"\n",
    "    log_entries = []\n",
    "\n",
    "    print(\"\\nðŸ“‹ Generated Steps:\")\n",
    "    for idx, step in enumerate(steps, start=1):\n",
    "        dry_run = False\n",
    "        # If first step looks like a shell command, echo it as dry-run\n",
    "        # Remove numbering for checking shell command\n",
    "        step_text = step.partition(\" \")[2].strip() if \" \" in step else step\n",
    "        if idx == 1 and is_shell_command(step_text):\n",
    "            dry_run = True\n",
    "            print(f\"(Dry-run) {step_text}\")\n",
    "        else:\n",
    "            print(f\"Step {idx}: {step_text}\")\n",
    "\n",
    "        log_entries.append({\n",
    "            \"step\": idx,\n",
    "            \"text\": step_text,\n",
    "            \"dry_run\": dry_run\n",
    "        })\n",
    "\n",
    "    with open(log_path, \"a\") as f:\n",
    "        for entry in log_entries:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "jgwbcpWBY36S",
    "outputId": "48a63f63-1a09-4a4e-cd68-d9b7111e58dc"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_63e2496f-5241-474f-90dd-8b3e87e91efc\", \"agent.py\", 3075)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('agent.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71_6TlUAcOn6"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./lora-gptneo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICNB8g8_chjC",
    "outputId": "28d53358-96d7-4b59-fdda-b964d88ca1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: lora-gptneo/ (stored 0%)\n",
      "  adding: lora-gptneo/README.md (deflated 66%)\n",
      "  adding: lora-gptneo/generation_config.json (deflated 24%)\n",
      "  adding: lora-gptneo/checkpoint-90/ (stored 0%)\n",
      "  adding: lora-gptneo/checkpoint-90/README.md (deflated 66%)\n",
      "  adding: lora-gptneo/checkpoint-90/optimizer.pt (deflated 8%)\n",
      "  adding: lora-gptneo/checkpoint-90/scheduler.pt (deflated 56%)\n",
      "  adding: lora-gptneo/checkpoint-90/generation_config.json (deflated 24%)\n",
      "  adding: lora-gptneo/checkpoint-90/trainer_state.json (deflated 71%)\n",
      "  adding: lora-gptneo/checkpoint-90/model.safetensors (deflated 8%)\n",
      "  adding: lora-gptneo/checkpoint-90/adapter_model.safetensors (deflated 7%)\n",
      "  adding: lora-gptneo/checkpoint-90/config.json (deflated 59%)\n",
      "  adding: lora-gptneo/checkpoint-90/training_args.bin (deflated 52%)\n",
      "  adding: lora-gptneo/checkpoint-90/adapter_config.json (deflated 54%)\n",
      "  adding: lora-gptneo/checkpoint-90/rng_state.pth (deflated 24%)\n",
      "  adding: lora-gptneo/model.safetensors (deflated 8%)\n",
      "  adding: lora-gptneo/adapter_model.safetensors (deflated 7%)\n",
      "  adding: lora-gptneo/config.json (deflated 59%)\n",
      "  adding: lora-gptneo/training_args.bin (deflated 52%)\n",
      "  adding: lora-gptneo/adapter_config.json (deflated 54%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r lora-gptneo.zip lora-gptneo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "-KSvWZIddJXI",
    "outputId": "3d86366f-a268-4c23-c717-636ddddcd7cd"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_e2202565-7c01-4219-80d6-2906af25743b\", \"lora-gptneo.zip\", 1847153798)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"lora-gptneo.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J8KLiloypAck",
    "outputId": "0c70002d-910b-4c73-abc8-1e07cf150aeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluate_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate_script.py\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Your 7 Q&A pairs for testing\n",
    "test_data = [\n",
    "  {\n",
    "    \"question\": \"Create a new Git branch and switch to it.\",\n",
    "    \"answer\": \"1. git branch <branch_name>\\n2. git checkout <branch_name>\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Compress the folder reports into reports.tar.gz.\",\n",
    "    \"answer\": \"1. tar -czvf reports.tar.gz reports\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"List all Python files in the current directory recursively.\",\n",
    "    \"answer\": \"1. find . -name '*.py'\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Set up a virtual environment and install requests.\",\n",
    "    \"answer\": \"1. python3 -m venv env\\n2. source env/bin/activate\\n3. pip install requests\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Fetch only the first ten lines of a file named output.log.\",\n",
    "    \"answer\": \"1. head -n 10 output.log\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How to check the size of a directory including all its contents?\",\n",
    "    \"answer\": \"1. du -sh <directory_name>\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How to find and delete all .tmp files in the current directory and its subdirectories?\",\n",
    "    \"answer\": \"1. find . -name '*.tmp' -type f -delete\"\n",
    "  }\n",
    "]\n",
    "\n",
    "# Load tokenizer and base model\n",
    "print(\"â³ Loading tokenizer and base model...\")\n",
    "base_model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "\n",
    "# Load fine-tuned LoRA adapter\n",
    "print(\"âœ… Loading fine-tuned LoRA adapter...\")\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./lora-gptneo\")\n",
    "lora_model.eval()\n",
    "\n",
    "# Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def generate_steps(model, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    steps_text = text.split(\"Steps:\")[-1].strip()\n",
    "    return steps_text\n",
    "\n",
    "print(\"\\nðŸ” Evaluating models on test data...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for entry in tqdm(test_data):\n",
    "    question = entry[\"question\"]\n",
    "    reference = entry[\"answer\"]\n",
    "\n",
    "    prompt = f\"Instruction: {question}\\nYou are an assistant that provides detailed step-by-step instructions.\\nPlease provide numbered steps to complete the instruction.\\nSteps:\"\n",
    "\n",
    "    # Generate outputs\n",
    "    base_gen = generate_steps(base_model, prompt)\n",
    "    lora_gen = generate_steps(lora_model, prompt)\n",
    "\n",
    "    # Avoid BLEU crash on empty strings\n",
    "    if not base_gen.strip():\n",
    "        print(f\"âš ï¸ Base model returned empty output for: {question}\")\n",
    "        base_gen = \"No output generated.\"\n",
    "\n",
    "    if not lora_gen.strip():\n",
    "        print(f\"âš ï¸ LoRA model returned empty output for: {question}\")\n",
    "        lora_gen = \"No output generated.\"\n",
    "\n",
    "    # Compute BLEU\n",
    "    base_bleu = bleu.compute(predictions=[base_gen], references=[[reference]])[\"bleu\"]\n",
    "    lora_bleu = bleu.compute(predictions=[lora_gen], references=[[reference]])[\"bleu\"]\n",
    "\n",
    "    # Compute ROUGE-L (returns float directly)\n",
    "    base_rouge = rouge.compute(predictions=[base_gen], references=[reference])[\"rougeL\"]\n",
    "    lora_rouge = rouge.compute(predictions=[lora_gen], references=[reference])[\"rougeL\"]\n",
    "\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"base_gen\": base_gen,\n",
    "        \"lora_gen\": lora_gen,\n",
    "        \"base_bleu\": base_bleu,\n",
    "        \"lora_bleu\": lora_bleu,\n",
    "        \"base_rouge\": base_rouge,\n",
    "        \"lora_rouge\": lora_rouge,\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for res in results:\n",
    "    print(f\"\\nQuestion: {res['question']}\\n\")\n",
    "    print(f\"Reference:\\n{res['reference']}\\n\")\n",
    "    print(f\"Base model output:\\n{res['base_gen']}\\nBLEU: {res['base_bleu']:.4f}  ROUGE-L: {res['base_rouge']:.4f}\\n\")\n",
    "    print(f\"LoRA model output:\\n{res['lora_gen']}\\nBLEU: {res['lora_bleu']:.4f}  ROUGE-L: {res['lora_rouge']:.4f}\\n\")\n",
    "\n",
    "# Save to JSON for record keeping\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete. Results saved to evaluation_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "j2rOZe4TpJgR",
    "outputId": "1425ff14-ebc4-4443-d55a-21cd8e8f08dc"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_a3821b26-1944-4e79-9a2a-c698ca2d3395\", \"evaluate_script.py\", 4195)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('evaluate_script.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCn2JgwxvoeT",
    "outputId": "be30c2a9-78f8-4722-e1bf-9223ad7e6093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import rouge_score\n",
    "import nltk\n",
    "import absl\n",
    "print(\"All packages imported successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
