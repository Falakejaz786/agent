# ğŸ–¥ï¸ Command-Line Q&A Model Fine-Tuning

This repository contains an end-to-end demo of fine-tuning a small language model (â‰¤ 2B parameters) on command-line Q&A pairs using LoRA, and wrapping it into a CLI assistant.

---

## ğŸš€ Project Overview

âœ… **Data:**  
Collected over 150 public Q&A pairs covering common command-line topics: Git, Bash, tar/gzip, grep, venv, and more.

âœ… **Model:**  
Fine-tuned `TinyLlama-1.1B` / `Phi-2` / `GPT-Neo-125M` (open weights, â‰¤ 2B parameters) using LoRA for 1 epoch on a free Colab T4 GPU.

âœ… **CLI Agent:**  
- Accepts a natural-language instruction.
- Generates a step-by-step plan using the fine-tuned model.
- Dry-runs shell commands (via `echo <command>`).
- Logs each step to `logs/trace.jsonl`.

âœ… **Evaluation:**  
- Compared base vs fine-tuned outputs on 7 prompts (5 predefined + 2 custom edge cases).
- BLEU / ROUGE-L scores + plan quality (0-2 scale).

---

## âš™ How to Use

### ğŸ”¹ Clone the repo
```bash
git clone https://github.com/Falakejaz786/agent.git
cd your-repo-name
````
### ğŸ”¹ Run the CLI agent

```bash
python agent.py
```

Type a natural-language task like:

```
create a new git branch called feature-login
```

The agent generates a plan, dry-runs commands (`echo <command>`), and logs to `logs/trace.jsonl`.

---

## ğŸ“Š Evaluation

* **Metrics:** BLEU / ROUGE-L
* **Plan Quality (0-2):**

  * 0 â†’ Poor/wrong plan
  * 1 â†’ Partial/incomplete plan
  * 2 â†’ Clear and correct plan

---

## ğŸ’¡ Future Improvements

* Extend dataset with harder multi-step command-line tasks.
* Add safe real command execution (sandboxed) instead of echo.

